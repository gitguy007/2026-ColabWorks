{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "48px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8NCVqhxKzVTo"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitguy007/2026-ColabWorks/blob/main/BizAnalytics_W02_Reuters_Authorship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqtE9qCqztTi"
      },
      "source": [
        "# Week 2: Python Business Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGotMBbXz1Pl"
      },
      "source": [
        "See the Repository for Future Work: https://github.com/firmai/python-business-analytics or\n",
        "\n",
        "Sign up to the mailing list: https://mailchi.mp/ec4942d52cc5/firmai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki1UWVLczVCC"
      },
      "source": [
        "# Text Mining NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeorQOsbzVCQ"
      },
      "source": [
        "# *Introduction*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeKqM74hzVCR"
      },
      "source": [
        "This notebook contains code examples to get you started with Natural Language Processing (NLP) / Text Mining for Research and Data Science purposes.  \n",
        "\n",
        "In the large scheme of things there are roughly 4 steps:  \n",
        "\n",
        "1. Identify a data source  \n",
        "2. Gather the data  \n",
        "3. Process the data  \n",
        "4. Analyze the data  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7XU4fuOzVCT"
      },
      "source": [
        "## Note: companion slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W8FwKUgzVCZ"
      },
      "source": [
        "# *Elements / topics that are discussed in this notebook: *\n",
        "\n",
        "\n",
        "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"50%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD7WDs30zVCa"
      },
      "source": [
        "# *Table of Contents*  <a id='toc'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqFVyXnDzVCc"
      },
      "source": [
        "* [Primer on NLP tools](#tool_primer)     \n",
        "* [Process + Clean text](#proc_clean)   \n",
        "    * [Normalization](#normalization)\n",
        "        * [Deal with unwanted characters](#unwanted_char)\n",
        "        * [Sentence segmentation](#sentence_seg)   \n",
        "        * [Word tokenization](#word_token)\n",
        "        * [Lemmatization & Stemming](#lem_and_stem)\n",
        "    * [Language modeling](#lang_model)\n",
        "        * [Part-of-Speech tagging](#pos_tagging)\n",
        "        * [Uni-Gram & N-Grams](#n_grams)\n",
        "        * [Stop words](#stop_words)\n",
        "* [Direct feature extraction](#feature_extract)\n",
        "    * [Feature search](#feature_search)\n",
        "        * [Entity recognition](#entity_recognition)\n",
        "        * [Pattern search](#pattern_search)\n",
        "    * [Text evaluation](#text_eval)\n",
        "        * [Language](#language)\n",
        "        * [Dictionary counting](#dict_counting)\n",
        "        * [Readability](#readability)\n",
        "* [Represent text numerically](#text_numerical)\n",
        "    * [Bag of Words](#bows)\n",
        "        * [TF-IDF](#tfidf)\n",
        "    * [Word Embeddings](#word_embed)\n",
        "        * [Word2Vec](#Word2Vec)\n",
        "* [Statistical models](#stat_models)\n",
        "    * [\"Traditional\" machine learning](#trad_ml)\n",
        "        * [Supervised](#trad_ml_supervised)\n",
        "            * [Naïve Bayes](#trad_ml_supervised_nb)\n",
        "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm)\n",
        "        * [Unsupervised](#trad_ml_unsupervised)\n",
        "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda)\n",
        "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis)\n",
        "* [Model Selection and Evaluation](#trad_ml_eval)\n",
        "* [Neural Networks](#nn_ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zJnQENtazVCf"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbPFc05MzVCh"
      },
      "source": [
        "There are many tools available for NLP purposes.  \n",
        "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
        "\n",
        "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
        "\n",
        "**Standard NLP libraries**:\n",
        "1. `Spacy` and the higher-level wrapper `Textacy`\n",
        "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
        "\n",
        "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
        "\n",
        "**Standard machine learning library**:\n",
        "\n",
        "1. `scikit learn`\n",
        "\n",
        "**Specific task libraries**:\n",
        "\n",
        "There are many, just a couple of examples:\n",
        "\n",
        "1. `pyLDAvis` for visualizing LDA)\n",
        "2. `langdetect` for detecting languages\n",
        "3. `fuzzywuzzy` for fuzzy text matching\n",
        "4. `textstat` to calculate readability statistics\n",
        "5. `Gensim` for topic modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Jj3UkiLSzVCj"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Get some example data</span><a id='example_data'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-N0OZKzVCm"
      },
      "source": [
        "There are many example datasets available to play around with, see for example this great repository:  \n",
        "https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9axpUNqqzVCp"
      },
      "source": [
        "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments.\n",
        "\n",
        "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_EQ6LkxzVCr"
      },
      "source": [
        "### Download and load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fPhFIZozVCt"
      },
      "source": [
        "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch) (although the `zipfile` and `io` operations are not very relevant)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ptU3H_7hdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fa1857-02b0-4bda-ca37-ea2e9162140e"
      },
      "source": [
        "!pip install textacy\n",
        "!pip install langdetect\n",
        "!pip install gensim\n",
        "!pip install fuzzywuzzy\n",
        "!pip install pyLDAvis"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (7.0.1)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.0.10)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (642 bytes)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.5.3)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.12/dist-packages (from textacy) (3.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.0.2)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.6.1)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (3.8.11)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.12/dist-packages (from textacy) (4.67.3)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (2026.1.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->textacy) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (2.5.2)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (0.23.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy~=3.0->textacy) (26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.4.2)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy~=3.0->textacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy~=3.0->textacy) (0.1.5)\n",
            "Requirement already satisfied: typer>=0.23.1 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (0.23.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy~=3.0->textacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy~=3.0->textacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy~=3.0->textacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy~=3.0->textacy) (2.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (0.1.2)\n",
            "Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading floret-0.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.6/321.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-1.1.0 floret-0.10.5 jellyfish-1.2.1 pyphen-0.17.2 textacy-0.13.0\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=fd960da652915368f92dffe33913a2dc0eb67f777933d6c7287bb0c3fd1a77d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (1.16.3)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (1.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (2.14.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim->pyLDAvis) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->pyLDAvis) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim->pyLDAvis) (2.1.1)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEwYopmNzVCv"
      },
      "source": [
        "import requests, zipfile, io, os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-8KFZ4qzVC-"
      },
      "source": [
        "*Download and extract the zip file with the data *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZjfB-bUzVDB"
      },
      "source": [
        "if not os.path.exists('C50test'):\n",
        "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVRLr8EbzVDR"
      },
      "source": [
        "*Load the data into memory*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzwaWd8HzVDT"
      },
      "source": [
        "folder_dict = {'test' : 'C50test'}\n",
        "text_dict = {'test' : {}}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USqLZVb4zVDg"
      },
      "source": [
        "for label, folder in folder_dict.items():\n",
        "    authors = os.listdir(folder)\n",
        "    for author in authors:\n",
        "        text_files = os.listdir(os.path.join(folder, author))\n",
        "        for file in text_files:\n",
        "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
        "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pbB8sAqzVEA"
      },
      "source": [
        "*Note: the text comes pre-split per sentence, for the sake of example I undo this through `' '.join(text_file.readlines()`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jZczhS4zVEI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "89d1a360-84bd-4313-bd09-74db1ef42139"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance.\\n Chief executive David Wellings said the group was trading in line with plans and still aimed to be one of the top three world confectionery firms by the end of the decade.\\n \"I think the word we used is robust. That means the business is performing to plan. The base business is performing well and our acquisitions are also growing to plan, especially Dr Pepper,\" he said in an interview.\\n For the half year to June 15, the company reported a 12 percent rise in pretax profits to 231 million pounds ($362 million) on sales up 13.3 percent to 2.293 billion pounds.\\n \"The outlook for the year as a whole is positive with further growth expected for both business streams,\" chairman Dominic Cadbury said in a statement.\\n Analysts\\' forecasts for pretax profits ranged from 218 million to 232 million pounds, after restructuring costs. The shares rose 11-1/2 pence to 523 on the news.\\n \"The results were at the upper end of the market range and will be well received,\" said Paribas analyst John Campbell.\\n The group\\'s confectionery business posted a three percent profits rise to 105 million in the half year while its drinks business chipped in 198 million, up from 165 million.\\n \"I think that is the pattern ahead, a base business growing well and a thrust into emerging markets at a pace which management believes is reasonable,\" he added.\\n Cadbury\\'s factories in Russia and China came on stream in the first half and Poland reached breakeven point in its second year of confectionery production.\\n \"We have a very clear objective in our confectionery stream and that is to be one of the top three by the year 2000. That means we need to sell one million tonnes of chocolate a year by the year 2000,\" Wellings said.\\n The company is the third largest soft drinks group in the world but trails in fourth spot behind Nestle, Mars and Suchard in confectionery.\\n The profits increase was driven by the first full six-month contribution from Dr Pepper/7Up -- the third largest soft drinks group in the United States -- acquired in January 1995 for $1.71 billion.\\n \"The acquisitions we made in 1995 are continuing to perform to expectations and that includes our Canadian acquisitions as well as Dr Pepper,\" he said.\\n Cadbury acquired Allan Candy for an undisclosed amount and Neilson Group for 108 million last year, making it the market leader in Canadian confectionery.\\n \"In North America both confectionery and soft drinks will continue to grow and that is about lifestyle and the breakdown of formal eating occasions and increased opportunities to snack,\" Wellings said.\\n \"So we will continue to see single-digit growth even in the mature markets like the U.S. and the U.K., but across Europe growth is going to be steady. The French, for instance, still tend to eat two large meals a day and resist the trend towards snacking.\"\\n Sales volumes of its 7Up soft drink were nearly two percent ahead of last year after a relaunch of its design and advertising but still behind Coca-Cola-owned Sprite as rivals stepped up their promotions.\\n When it was first acquired, 7Up had level pegging with Sprite and Cadbury is trying hard to turn the brand around. The Dr Pepper brand, however, is outpacing growth in the U.S. market.\\n The company agreed to sell its 51-percent stake in Coca-Cola &amp; Schweppes Beverages for 622.5 million, which will greatly reduce debt and provide more scope for acquisitions, although Wellings declined to be drawn on what they might be.\\n The group set aside 35 million pounds for restructuring of its Schweppes France operation -- reflecting the decision to set up in partnership with San Benedetto, the Italian producer of mineral water and soft drinks -- to build a new plant and close down two existing older ones. ($1=.6382 Pound)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hcDfreU4zVE4"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Process + Clean text</span><a id='proc_clean'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-wjbBW-zVE5"
      },
      "source": [
        "## Convert the text into a NLP representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDY9_DR8zVFA"
      },
      "source": [
        "We can use the text directly, but if want to use packages like `spacy` and `textblob` we first have to convert the text into a corresponding object.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP_gh4-XzVFC"
      },
      "source": [
        "### Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6lbCCb4zVFE"
      },
      "source": [
        "**Note:** depending on the way that you installed the language models you will need to import it differently:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z7q_0kpzVFG"
      },
      "source": [
        "```\n",
        "from spacy.en import English\n",
        "parser = English()\n",
        "```\n",
        "OR\n",
        "```\n",
        "import en_core_web_sm\n",
        "parser = en_core_web_sm.load()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj4GrH-1zVFI"
      },
      "source": [
        "import en_core_web_sm\n",
        "parser = en_core_web_sm.load()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE4wAqeszVFW"
      },
      "source": [
        "Convert all text in the \"test\" sample to a `spacy` `doc` object using `parser()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XCZmFbzVFY"
      },
      "source": [
        "spacy_text = {}\n",
        "for author, text_list in text_dict['test'].items():\n",
        "    spacy_text[author] = [parser(text) for text in text_list]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUP_HWzxzVFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8324c6-0362-4af6-bdc8-3c318243c668"
      },
      "source": [
        "type(spacy_text['TimFarrand'][0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gR79Z6jzVFy"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im0zp4IKzVF0"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_BSfJKezVF7"
      },
      "source": [
        "We can apply basic `nltk` operations directly to the text so we don't need to convert first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzbRGbaazVGD"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVYhYEDUzVGI"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h9cyqnjzVGT"
      },
      "source": [
        "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miCnaBPCzVGY"
      },
      "source": [
        "textblob_text = {}\n",
        "for author, text_list in text_dict['test'].items():\n",
        "    textblob_text[author] = [TextBlob(text) for text in text_list]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_STJ4QWizVGr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f6141918-1bc3-447b-bd5e-aa5d5e39226e"
      },
      "source": [
        "type(textblob_text['TimFarrand'][0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "textblob.blob.TextBlob"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>textblob.blob.TextBlob</b><br/>def __init__(text, tokenizer=None, pos_tagger=None, np_extractor=None, analyzer=None, parser=None, classifier=None, clean_html=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/textblob/blob.py</a>A general text block, meant for larger bodies of text (esp. those\n",
              "containing sentences). Inherits from :class:`BaseBlob &lt;BaseBlob&gt;`.\n",
              "\n",
              ":param str text: A string.\n",
              ":param tokenizer: (optional) A tokenizer instance. If ``None``, defaults to\n",
              "    :class:`WordTokenizer() &lt;textblob.tokenizers.WordTokenizer&gt;`.\n",
              ":param np_extractor: (optional) An NPExtractor instance. If ``None``,\n",
              "    defaults to :class:`FastNPExtractor() &lt;textblob.en.np_extractors.FastNPExtractor&gt;`.\n",
              ":param pos_tagger: (optional) A Tagger instance. If ``None``, defaults to\n",
              "    :class:`NLTKTagger &lt;textblob.en.taggers.NLTKTagger&gt;`.\n",
              ":param analyzer: (optional) A sentiment analyzer. If ``None``, defaults to\n",
              "    :class:`PatternAnalyzer &lt;textblob.en.sentiments.PatternAnalyzer&gt;`.\n",
              ":param classifier: (optional) A classifier.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 596);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "s3eNuaFczVG0"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHlPtBa6zVG4"
      },
      "source": [
        "**Text normalization** describes the task of transforming the text into a different (more comparable) form.  \n",
        "\n",
        "This can imply many things, I will show a couple of things below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Gui1Vbh-zVG9"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Deal with unwanted characters</span><a id='unwanted_char'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJqf_-unzVG_"
      },
      "source": [
        "You will often notice that there are characters that you don't want in your text.  \n",
        "\n",
        "Let's look at this sentence for example:\n",
        "\n",
        "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
        "\n",
        "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI528gkQzVHD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f13635b5-91ee-4c76-cb5c-d01d2db2d020"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0][:298]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance.\\n Chief executive David Wellings said the group was trading in line with plans and still aimed to be one of the top three world c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ot36oM-YzVHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30fe119-2aed-4f8a-bebf-46045fb4c94f"
      },
      "source": [
        "print(text_dict['test']['TimFarrand'][0][:298])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance.\n",
            " Chief executive David Wellings said the group was trading in line with plans and still aimed to be one of the top three world c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48pFgqarzVHU"
      },
      "source": [
        "If we want to analyze text we often don't care about the visual representation. They might actually cause problems!  \n",
        "\n",
        "** So how do we remove them? **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foq49UGGzVHX"
      },
      "source": [
        "In many cases it is sufficient to simply use the `.replace()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjf30lSZzVHZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2ee6f59-81c7-4cab-ba3a-301efc903bd3"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance. Chief executive David Wellings said the group was trading in line with plans and still aimed to be one of the top three world c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyl9zCsAzVHn"
      },
      "source": [
        "Sometimes, however, the problem arrises because of encoding / decoding problems.  \n",
        "\n",
        "In those cases you can usually do something like:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RGwL1nKzVHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b68fa99-2237-4008-d803-ecae9110d6a9"
      },
      "source": [
        "problem_sentence = 'This is some \\\\u03c0 text that has to be cleaned\\\\u2026! it\\\\u0027s annoying!'\n",
        "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"This is some  text that has to be cleaned! it's annoying!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hO0E7tL9zVH0"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Sentence segmentation</span><a id='sentence_seg'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B7YG1mczVH-"
      },
      "source": [
        "Sentence segmentation means the task of splitting up the piece of text by sentence.  \n",
        "\n",
        "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ufvwboczVIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d7b578-124d-4dee-cfe6-fcba343354df"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0][:550].split('.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance',\n",
              " '\\n Chief executive David Wellings said the group was trading in line with plans and still aimed to be one of the top three world confectionery firms by the end of the decade',\n",
              " '\\n \"I think the word we used is robust',\n",
              " ' That means the business is performing to plan',\n",
              " ' The base business is performing well and our acquisitions are also growing to plan, especially Dr Pepper,\" he said in an i']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXXcCidzVIq"
      },
      "source": [
        "It is better to use a more sophisticated implementation such as the one by `Spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhqki3D5zVIr"
      },
      "source": [
        "example_paragraph = spacy_text['TimFarrand'][0]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpMnK7ZdzVIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fa5fb3-7110-4cca-d928-0835c754df17"
      },
      "source": [
        "sentence_list = [s for s in example_paragraph.sents]\n",
        "sentence_list[:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance.\n",
              "  ,\n",
              " Chief executive David Wellings said the group was trading in line with plans and still aimed to be one of the top three world confectionery firms by the end of the decade.\n",
              "  ,\n",
              " \"I think the word we used is robust.,\n",
              " That means the business is performing to plan.,\n",
              " The base business is performing well and our acquisitions are also growing to plan, especially Dr Pepper,\" he said in an interview.\n",
              "  ]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMEdmpOozVI_"
      },
      "source": [
        "Notice that the returned object is still a `spacy` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-t-CTMLzVJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c730dbe9-4346-4fd2-b290-d3144b774379"
      },
      "source": [
        "type(sentence_list[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hubnl9KzVJN"
      },
      "source": [
        "Apply to all texts (for use later on):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4ldqRWPzVJP"
      },
      "source": [
        "spacy_sentences = {}\n",
        "for author, text_list in spacy_text.items():\n",
        "    spacy_sentences[author] = [list(text.sents) for text in text_list]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK4F1nTCzVJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b487f126-3e50-41a1-914b-e587a5332133"
      },
      "source": [
        "spacy_sentences['TimFarrand'][0][:3]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance.\n",
              "  ,\n",
              " Chief executive David Wellings said the group was trading in line with plans and still aimed to be one of the top three world confectionery firms by the end of the decade.\n",
              "  ,\n",
              " \"I think the word we used is robust.]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "pnFio7DDzVJm"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Word tokenization</span><a id='word_token'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHvcdrOfzVJp"
      },
      "source": [
        "Word tokenization means to split the sentence (or text) up into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7yhJWfJzVJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d0d6be-b453-4437-c831-2ae692fe2ce2"
      },
      "source": [
        "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
        "example_sentence"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Confectionery and soft drinks group Cadbury-Schweppes on Wednesday reported healthy first-half results and said it was positive about its expected full-year performance.\n",
              " "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ubhNWlnzVKj"
      },
      "source": [
        "A word is called a `token` in this context (hence `tokenization`), using `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NyYQtZLzVKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe04f4af-e4dd-4800-e84d-cbd1061b6945"
      },
      "source": [
        "token_list = [token for token in example_sentence]\n",
        "token_list[0:15]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Confectionery,\n",
              " and,\n",
              " soft,\n",
              " drinks,\n",
              " group,\n",
              " Cadbury,\n",
              " -,\n",
              " Schweppes,\n",
              " on,\n",
              " Wednesday,\n",
              " reported,\n",
              " healthy,\n",
              " first,\n",
              " -,\n",
              " half]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "289WZS32zVKv"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Sqgia2zVK0"
      },
      "source": [
        "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
        "\n",
        "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
        "\n",
        "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0R7wbGxzVK5"
      },
      "source": [
        "**Spacy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWo_LBdNzVLB"
      },
      "source": [
        "Space offers build-in functionality for lemmatization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjeVMKvDzVLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf630d2-ce7e-4415-81f8-7dc5452fb8c9"
      },
      "source": [
        "lemmatized = [token.lemma_ for token in example_sentence]\n",
        "lemmatized[0:15]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['confectionery',\n",
              " 'and',\n",
              " 'soft',\n",
              " 'drink',\n",
              " 'group',\n",
              " 'Cadbury',\n",
              " '-',\n",
              " 'Schweppes',\n",
              " 'on',\n",
              " 'Wednesday',\n",
              " 'report',\n",
              " 'healthy',\n",
              " 'first',\n",
              " '-',\n",
              " 'half']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUnihWkKzVLa"
      },
      "source": [
        "**NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWRNB4e7zVLd"
      },
      "source": [
        "Using the NLTK libary we can also use the more aggressive Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEqxYGvszVLe"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yMJq8eKzVLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a5e456-5c41-4d37-f027-d0aa5cce0d8c"
      },
      "source": [
        "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
        "stemmed[0:15]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['confectioneri',\n",
              " 'and',\n",
              " 'soft',\n",
              " 'drink',\n",
              " 'group',\n",
              " 'cadburi',\n",
              " '-',\n",
              " 'schwepp',\n",
              " 'on',\n",
              " 'wednesday',\n",
              " 'report',\n",
              " 'healthi',\n",
              " 'first',\n",
              " '-',\n",
              " 'half']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOw9KWKbzVLz"
      },
      "source": [
        "**Compare**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWzVOxHMzVL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3686900-97c5-4cae-af63-033fec3e06b2"
      },
      "source": [
        "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
        "    print(original, ' | ', lemma, ' | ', stem)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confectionery  |  confectionery  |  confectioneri\n",
            "and  |  and  |  and\n",
            "soft  |  soft  |  soft\n",
            "drinks  |  drink  |  drink\n",
            "group  |  group  |  group\n",
            "Cadbury  |  Cadbury  |  cadburi\n",
            "-  |  -  |  -\n",
            "Schweppes  |  Schweppes  |  schwepp\n",
            "on  |  on  |  on\n",
            "Wednesday  |  Wednesday  |  wednesday\n",
            "reported  |  report  |  report\n",
            "healthy  |  healthy  |  healthi\n",
            "first  |  first  |  first\n",
            "-  |  -  |  -\n",
            "half  |  half  |  half\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WST1UsmzVMA"
      },
      "source": [
        "In my experience it is usually best to use lemmatization instead of a stemmer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "k1n7zXYRzVMC"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Language modeling</span><a id='lang_model'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOTi2TTEzVMD"
      },
      "source": [
        "Text is inherently structured in complex ways, we can often use some of this underlying structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "OEiCmMoVzVMH"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging</span><a id='pos_tagging'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvGsa0DzVMI"
      },
      "source": [
        "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONuqyRPvzVMQ"
      },
      "source": [
        "Using `Spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4B7AUIxzVMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c26247-32f5-4281-aa45-01a90581fc51"
      },
      "source": [
        "pos_list = [(token, token.pos_) for token in example_sentence]\n",
        "pos_list[0:10]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Confectionery, 'NOUN'),\n",
              " (and, 'CCONJ'),\n",
              " (soft, 'ADJ'),\n",
              " (drinks, 'NOUN'),\n",
              " (group, 'NOUN'),\n",
              " (Cadbury, 'PROPN'),\n",
              " (-, 'PUNCT'),\n",
              " (Schweppes, 'PROPN'),\n",
              " (on, 'ADP'),\n",
              " (Wednesday, 'PROPN')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "D152zwLUzVMX"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-gWZCBzVMY"
      },
      "source": [
        "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
        "\n",
        "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
        "An `n-gram` is nothing more than a a combination of `N` words into one token (a uni-gram token is just one word).  \n",
        "\n",
        "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
        "\n",
        "> Sentence-about, about-flying, flying-cars  \n",
        "\n",
        "See my slide on N-Grams for a more comprehensive example: [click here](http://www.tiesdekok.com/AccountingNLP_Slides/#14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pc-zTsdzVMZ"
      },
      "source": [
        "Using `NLTK`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNY2U2VyzVMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280f8dc5-06df-4317-cc81-f26b74f081ce"
      },
      "source": [
        "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
        "bigram_list[10:15]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['reported-healthy', 'healthy-first', 'first--', '--half', 'half-results']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jgy4iAQpzVMj"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Stop words</span><a id='stop_words'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bif0cWA9zVMl"
      },
      "source": [
        "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
        "\n",
        "The primary example are stop words.  \n",
        "\n",
        "Sometimes you can improve the accuracy of your model by removing stop words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9m4Rq7QzVMo"
      },
      "source": [
        "Using `Spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7w9yH9hzVMp"
      },
      "source": [
        "no_stop_words = [token for token in example_sentence if not token.is_stop]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1sYIDe-zVMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88b54a7-10c0-446c-b42b-e1f64d9e406b"
      },
      "source": [
        "no_stop_words[:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Confectionery,\n",
              " soft,\n",
              " drinks,\n",
              " group,\n",
              " Cadbury,\n",
              " -,\n",
              " Schweppes,\n",
              " Wednesday,\n",
              " reported,\n",
              " healthy]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpmZjNLXzVM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8af0b1-bd6b-4d2c-97e4-5809b8ef6f6e"
      },
      "source": [
        "token_list[:10]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Confectionery, and, soft, drinks, group, Cadbury, -, Schweppes, on, Wednesday]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfyZoy-6zVNM"
      },
      "source": [
        "*Note* we can also remove punctuation in the same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kd_rGgTzVNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb35ee89-4002-46e8-ddd1-d39d3b0a8fea"
      },
      "source": [
        "[token for token in example_sentence if not token.is_punct][:5]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Confectionery, and, soft, drinks, group]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnGJNZJWzVNV"
      },
      "source": [
        "## Wrap everything into one function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN5Ti3nIzVNW"
      },
      "source": [
        "Below I will primarily use `SpaCy` directly. However, I also recommend to check out the high-level wrapper `Textacy`.\n",
        "\n",
        "See their GitHub page for details: https://github.com/chartbeat-labs/textacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZKHceWXzVNX"
      },
      "source": [
        "### Quick `Textacy` example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCVsa9_Q6CmU"
      },
      "source": [],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikof63OBzVNY"
      },
      "source": [
        "import textacy"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwjvM34vzVNe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "458ee475-866f-498c-f0b8-09a4d6a27698"
      },
      "source": [
        "example_text = text_dict['test']['TimFarrand'][0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text_dict' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1134372229.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TimFarrand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text_dict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDUV2X1LzVNj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4829e730-3ef7-41b2-f78f-f66c07128531"
      },
      "source": [
        "import textacy.preprocessing.normalize\n",
        "\n",
        "cleaned_text = example_text\n",
        "# Fix unicode characters\n",
        "cleaned_text = textacy.preprocessing.normalize.normalize_unicode(cleaned_text)\n",
        "# Convert to lowercase\n",
        "cleaned_text = cleaned_text.lower()\n",
        "# Remove punctuation and replace with spaces\n",
        "cleaned_text = textacy.preprocessing.normalize.remove_punct_with_space(cleaned_text)\n",
        "# Optionally, normalize whitespace after punctuation removal\n",
        "cleaned_text = textacy.preprocessing.normalize.normalize_whitespace(cleaned_text)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'example_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1574953252.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fix unicode characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'example_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M16-lmobzVNo"
      },
      "source": [
        "** Basic SpaCy text processing function **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R8Q6c5rzVNq"
      },
      "source": [
        "1. Split into sentences\n",
        "2. Apply lemmatizer and remove top words\n",
        "3. Clean up the sentence using `textacy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk2lMON0zVNs"
      },
      "source": [
        "def process_text_custom(text):\n",
        "    sentences = list(parser(text).sents)\n",
        "    lemmatized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        lemmatized_sentences.append([token.lemma_ for token in sentence if not token.is_stop | token.is_punct | token.is_space])\n",
        "    return [parser(' '.join(sentence)) for sentence in lemmatized_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HwX9R_YzVNw"
      },
      "source": [
        "%%time\n",
        "spacy_text_clean = {}\n",
        "for author, text_list in text_dict['test'].items():\n",
        "    lst = []\n",
        "    for text in text_list:\n",
        "        lst.append(process_text_custom(text))\n",
        "    spacy_text_clean[author] = lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OK3ybv1zVN5"
      },
      "source": [
        "Note that there are quite a lot of sentences (~52K) so this takes a bit of time (~ 15 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yLy5CSlzVN6"
      },
      "source": [
        "count = 0\n",
        "for author, texts in spacy_text_clean.items():\n",
        "    for text in texts:\n",
        "        count += len(text)\n",
        "print('Number of sentences:', count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov_YJypWzVOO"
      },
      "source": [
        "Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABxccKHezVOP"
      },
      "source": [
        "spacy_text_clean['TimFarrand'][0][:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Uo89NizNzVOY"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Direct feature extraction</span><a id='feature_extract'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJMPsyLwzVOZ"
      },
      "source": [
        "We now have pre-processed our text into something that we can use for direct feature extraction or to convert it to a numerical representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "vefZY_WbzVOa"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Feature search</span><a id='feature_search'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zvB_TQqwzVOb"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Entity recognition</span><a id='entity_recognition'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkHbQjBWzVOc"
      },
      "source": [
        "It is often useful / relevant to extract entities that are mentioned in a piece of text.   \n",
        "\n",
        "SpaCy is quite powerful in extracting entities, however, it doesn't work very well on lowercase text.  \n",
        "\n",
        "Given that \"token.lemma\\_\" removes capitalization I will use `spacy_sentences` for this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-57j90XDzVOd"
      },
      "source": [
        "example_sentence = spacy_sentences['TimFarrand'][0][3]\n",
        "example_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUFq1a10zVOp"
      },
      "source": [
        "[(i, i.label_) for i in parser(example_sentence.text).ents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFlwjICAzVOv"
      },
      "source": [
        "example_sentence = spacy_sentences['TimFarrand'][4][0]\n",
        "example_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTM4MjgTzVO5"
      },
      "source": [
        "[(i, i.label_) for i in parser(example_sentence.text).ents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Phyb4rbpzVPM"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Pattern search</span><a id='pattern_search'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Odw0uoyzVPN"
      },
      "source": [
        "Using the build-in `re` (regular expression) library you can pattern match nearly anything you want.  \n",
        "\n",
        "I will not go into details about regular expressions but see here for a tutorial:  \n",
        "https://regexone.com/references/python  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZs3HpaAzVPO"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PW3WB9zVPU"
      },
      "source": [
        "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
        "\n",
        "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFiKKAZrzVPX"
      },
      "source": [
        "**Example 1:**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5_ek_NkzVPY"
      },
      "source": [
        "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
        "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywReAZ9IzVPd"
      },
      "source": [
        "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXyhq2CIzVPh"
      },
      "source": [
        "print(re.findall(pattern, string_1)[0])\n",
        "print(re.findall(pattern, string_2)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_vzcmowzVQL"
      },
      "source": [
        "### Example 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjI6qqkzVQM"
      },
      "source": [
        "If a sentence contains the word 'million' return True, otherwise return False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RhtvMohizVQP"
      },
      "source": [
        "for sen in spacy_text_clean['TimFarrand'][2]:\n",
        "    TERM = 'million'\n",
        "    contains = True if re.search('million', sen.text) else False\n",
        "    if contains:\n",
        "        print(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "_kAH64jTzVQc"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Text evaluation</span><a id='text_eval'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGXV6jb_zVQd"
      },
      "source": [
        "Besides feature search there are also many ways to analyze the text as a whole.  \n",
        "\n",
        "Let's, for example, evaluate the following paragraph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1waxmozVQf"
      },
      "source": [
        "example_paragraph = ' '.join([x.text for x in spacy_text_clean['TimFarrand'][2]])\n",
        "example_paragraph[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "7CbA_Uj4zVQv"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Language</span><a id='language'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-h5gNZHzVQw"
      },
      "source": [
        "Using the `langdetect` package it is easy to detect the language of a piece of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NbTq0U3_IVA"
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAEVJFYyzVQx"
      },
      "source": [
        "from langdetect import detect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlqALfz7zVQ0"
      },
      "source": [
        "detect(example_paragraph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "frmtGDTPzVRE"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Readability</span><a id='readability'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKalz3yzVRN"
      },
      "source": [
        "Using the `textstat` package we can compute various readability metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWC-8mwTzVRO"
      },
      "source": [
        "https://github.com/shivam5992/textstat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPo_GBg7C5CM"
      },
      "source": [
        "!pip install textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVTjORoHzVRO"
      },
      "source": [
        "from textstat.textstat import textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gc9LPUVHzVRS"
      },
      "source": [
        "print(textstat.flesch_reading_ease(example_paragraph))\n",
        "print(textstat.smog_index(example_paragraph))\n",
        "print(textstat.flesch_kincaid_grade(example_paragraph))\n",
        "print(textstat.coleman_liau_index(example_paragraph))\n",
        "print(textstat.automated_readability_index(example_paragraph))\n",
        "print(textstat.dale_chall_readability_score(example_paragraph))\n",
        "print(textstat.difficult_words(example_paragraph))\n",
        "print(textstat.linsear_write_formula(example_paragraph))\n",
        "print(textstat.gunning_fog(example_paragraph))\n",
        "print(textstat.text_standard(example_paragraph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPO_VVHrzVRZ"
      },
      "source": [
        "## Text similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07d0tYdGDCn-"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install fuzzywuzzy\n",
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m81Jm4m4zVRf"
      },
      "source": [
        "from fuzzywuzzy import fuzz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2vblIgozVRi",
        "outputId": "db8a2323-fc05-4840-88d0-ad0d2580e133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "a6c5W0NVzVRn"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Term (dictionary) counting</span><a id='dict_counting'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNiB1uqzVRo"
      },
      "source": [
        "One of the most common techniques that researchers currently use (at least in Accounting research) are simple metrics based on counting words in a dictionary.  \n",
        "This technique is, for example, very prevalent in sentiment analysis (counting positive and negative words).  \n",
        "\n",
        "In essence this technique is very simple to program:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ_Gm7v-zVRp"
      },
      "source": [
        "### Example 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WboJdEXxzVRs"
      },
      "source": [
        "word_dictionary = ['soft', 'first', 'most', 'be']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZFDFOIezVRv",
        "outputId": "c3230d09-7438-475b-a3d5-9879880fae75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "for word in word_dictionary:\n",
        "    print(word, example_paragraph.count(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "soft 0\n",
            "first 0\n",
            "most 0\n",
            "be 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGv6r4tAzVR9"
      },
      "source": [
        "### Example 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45YS4EmDzVR-",
        "outputId": "abdb881c-f759-4d0f-f626-a702615d424a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "pos = ['great', 'increase']\n",
        "neg = ['bad', 'decrease']\n",
        "\n",
        "sentence = '''According to Trump everything is great, great,\n",
        "and great even though his popularity is seeing a decrease.'''\n",
        "\n",
        "pos_count = 0\n",
        "for word in pos:\n",
        "    pos_count += sentence.lower().count(word)\n",
        "print(pos_count)\n",
        "\n",
        "neg_count = 0\n",
        "for word in neg:\n",
        "    neg_count += sentence.lower().count(word)\n",
        "print(neg_count)\n",
        "\n",
        "pos_count / (neg_count + pos_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TybOMjAQzVSH"
      },
      "source": [
        "sentence = '''According to Trump everything is great, great,\n",
        "and great even though his popularity is seeing a decrease.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCBJuvipzVSK",
        "outputId": "9121dd43-79f5-448f-98fe-dae23503b390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos_count = 0\n",
        "for word in pos:\n",
        "    pos_count += sentence.lower().count(word)\n",
        "print(pos_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by03-OIVzVSR",
        "outputId": "e223d80b-d636-4a9f-c537-d9ab5850cd28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_count = 0\n",
        "for word in neg:\n",
        "    neg_count += sentence.lower().count(word)\n",
        "print(neg_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMiSUvOYzVSX",
        "outputId": "652a7451-3cce-4167-d3e2-5a3276a10f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos_count / (neg_count + pos_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCmhJIbszVSc"
      },
      "source": [
        "Getting the total number of words is also easy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj1GiPsxzVSd",
        "outputId": "492bbe68-1976-4c9b-f4bf-7c02901c68ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(parser(example_paragraph))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "370"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NCVqhxKzVTo"
      },
      "source": [
        "#### Example 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYMkGWDtzVTo"
      },
      "source": [
        "We can also save the count per word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV75kU7nzVTq"
      },
      "source": [
        "pos_count_dict = {}\n",
        "for word in pos:\n",
        "    pos_count_dict[word] = sentence.lower().count(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ctUf44xzVTt",
        "outputId": "5eaca5e7-9ac4-4019-9d5f-6f870a6f7c65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pos_count_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'great': 3, 'increase': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HV_eHMHxzVUu"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Represent text numerically</span><a id='text_numerical'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Or4hmJKyzVVC"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_cQOeC2zVVH"
      },
      "source": [
        "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
        "\n",
        "For details, see the documentation:  \n",
        "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
        "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
        "\n",
        "Note 1: these functions also already includes a lot of preprocessing options (e.g. ngrames, remove stop words, accent stripper).\n",
        "\n",
        "Note 2: example based on the following website [click here](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo5GO5o-zVVY"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR6YA_aUzVVe"
      },
      "source": [
        "### Simple example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpy0oD0NzVVf"
      },
      "source": [
        "doc_1 = \"The sky is blue.\"\n",
        "doc_2 = \"The sun is bright today.\"\n",
        "doc_3 = \"The sun in the sky is bright.\"\n",
        "doc_4 = \"We can see the shining sun, the bright sun.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVTX0FTezVVp"
      },
      "source": [
        "Calculate term frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wMnrgmlzVVq"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "tf = vectorizer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lm2LlYNzVVy",
        "outputId": "a274f7b1-4d5f-4c39-dddb-8b6f0d5c0f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(vectorizer.get_feature_names())\n",
        "for doc_tf_vector in tf.toarray():\n",
        "    print(doc_tf_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['blue', 'bright', 'shining', 'sky', 'sun', 'today']\n",
            "[1 0 0 1 0 0]\n",
            "[0 1 0 0 1 1]\n",
            "[0 1 0 1 1 0]\n",
            "[0 1 1 0 2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "GPRbzfvJzVWT"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQR-cppDzVWU"
      },
      "source": [
        "transformer = TfidfVectorizer(stop_words='english')\n",
        "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "DP7xLrCKzVWY",
        "outputId": "114f3664-b792-404d-c230-cc3cd4107ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "for doc_vector in tfidf.toarray():\n",
        "    print(doc_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.78528828 0.         0.         0.6191303  0.         0.        ]\n",
            "[0.         0.47380449 0.         0.         0.47380449 0.74230628]\n",
            "[0.         0.53256952 0.         0.65782931 0.53256952 0.        ]\n",
            "[0.         0.36626037 0.57381765 0.         0.73252075 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRCeQpwVzVWd"
      },
      "source": [
        "### More elaborate example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLQYa1iFzVWe"
      },
      "source": [
        "clean_paragraphs = []\n",
        "for author, value in spacy_text_clean.items():\n",
        "    for article in value:\n",
        "        clean_paragraphs.append(' '.join([x.text for x in article]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XueJvbeZzVWh",
        "outputId": "a408eada-ea06-496a-c7c5-6cdbab7a401b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(clean_paragraphs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99XHAHCFzVWl"
      },
      "source": [
        "transformer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_large = transformer.fit_transform(clean_paragraphs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5oOQ889zVWp",
        "outputId": "1d1acfd4-66f1-4f3a-f2ef-3392878b26f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Number of vectors:', len(tfidf_large.toarray()))\n",
        "print('Number of words in dictionary:', len(tfidf_large.toarray()[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of vectors: 2500\n",
            "Number of words in dictionary: 24130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akku50qvzVWt",
        "outputId": "b239fb66-e499-4465-d6a3-935a1cd056ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tfidf_large"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2500x24130 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 444178 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "89OXigz8zVWy"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VxwA3sptzVWz"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpmYO8izVW0"
      },
      "source": [
        "Simple example below is from:  https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8RousiXzVW1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a3c6220a-e17d-43a4-d64f-2582eac73515"
      },
      "source": [
        "import gensim\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPBPtxWqzVW_"
      },
      "source": [
        "sentences = brown.sents()\n",
        "model = gensim.models.Word2Vec(sentences, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXtzmP__zVXE"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_TJbkwzVXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1f285964-91d3-4ce0-a503-c975d13cbf88"
      },
      "source": [
        "model.save('brown_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI-mzHzkzVXT"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-BUV7FPzVXW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fb21fdd6-bd26-4318-fd44-18b63572c0d7"
      },
      "source": [
        "model = gensim.models.Word2Vec.load('brown_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ81a8jPzVXd"
      },
      "source": [
        "Find words most similar to 'mother':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Dk7RBgzVXd",
        "outputId": "50dba550-4078-4c3a-9536-e72c3c977815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(model.most_similar(\"mother\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('father', 0.9841663837432861), ('husband', 0.9670721292495728), ('wife', 0.9487313032150269), ('friend', 0.9323579668998718), ('son', 0.9275298714637756), ('nickname', 0.9200363159179688), ('eagle', 0.9182674288749695), ('addiction', 0.9054847955703735), ('voice', 0.9040984511375427), ('patient', 0.8997060060501099)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmI5IFgmzVXh"
      },
      "source": [
        "Find the odd one out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-mbhBujzVXi",
        "outputId": "30d71cdb-66a8-4829-a592-d47b9ccb4663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cereal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nq_qc0AzVXx",
        "outputId": "9c69724d-8635-4b1d-8ba6-bac69e3cd5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "print(model.doesnt_match(\"pizza pasta garden fries\".split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "garden\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8NJT85TzVX4"
      },
      "source": [
        "Retrieve vector representation of the word \"human\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84jajA9wzVX5",
        "outputId": "55fbab38-0c14-4362-f705-0253589551e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "model['human']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.2541619 ,  2.2159684 ,  1.047784  , -0.71238947, -0.61424005,\n",
              "       -0.2838337 ,  0.22731484,  0.64372134, -0.5807566 , -0.2922792 ,\n",
              "       -0.4993856 ,  0.19945972,  0.5109125 , -0.44036752, -0.31287178,\n",
              "       -0.05132972,  1.0744963 ,  0.49401098, -0.7575154 , -0.7520932 ,\n",
              "       -0.47252244,  0.15777719,  0.01892141, -0.13521086,  0.13225318,\n",
              "        0.01569277,  0.8201043 , -0.18367213, -0.5115785 ,  0.0950046 ,\n",
              "       -0.901529  , -0.00576999, -0.18296817, -0.7227348 , -0.36876544,\n",
              "        0.12904815, -0.49410135, -0.16567625,  0.25800195, -0.9476387 ,\n",
              "        0.6677448 , -0.66520613,  0.15521108, -0.05746429, -0.66669667,\n",
              "        1.1473489 , -0.30393326,  0.27609795,  0.03071395,  0.21279913,\n",
              "       -0.15011618,  0.06648927,  0.4653522 , -0.06295931, -0.59686804,\n",
              "        0.22332567,  0.52038115,  0.08707199, -0.03864726, -1.1777682 ,\n",
              "       -0.6586736 ,  0.7845623 ,  0.54146487, -1.0455779 , -0.5684107 ,\n",
              "        0.0466442 ,  0.44047025, -0.28766677, -0.17281069, -0.18058509,\n",
              "        0.21136013,  0.95359594, -0.66299623, -0.28144175,  0.01736428,\n",
              "        0.9233736 ,  0.533335  ,  0.47873688,  0.30168334, -1.0639472 ,\n",
              "        0.03699052,  0.32431152,  0.9761932 ,  0.6572108 , -0.32395357,\n",
              "       -0.21990493,  0.82805634,  0.27187476, -0.50445306,  0.06266934,\n",
              "       -0.05255144,  0.45138708,  0.34316576, -0.5600966 , -0.52912354,\n",
              "        0.18519725, -0.36004648, -0.4918508 , -0.04166657,  0.17255953],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6Eht6I1JzVYG"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Statistical models</span><a id='stat_models'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iQTGfmcHzVYH"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paklyzdrzVYI"
      },
      "source": [
        "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "mY1gvLnozVYI"
      },
      "source": [
        "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyvdgww1zVYJ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.externals import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47LgZmhMzVYS"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyNejarAzVYr"
      },
      "source": [
        "### Convert the data into a pandas dataframe (so that we can input it easier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwRfyuMazVYv"
      },
      "source": [
        "article_list = []\n",
        "for author, value in spacy_text_clean.items():\n",
        "    for article in value:\n",
        "        article_list.append((author, ' '.join([x.text for x in article])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjMTWkorzVYz"
      },
      "source": [
        "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukjf2aY0zVY2",
        "outputId": "47e30415-9e0d-40d5-ee99-17a54813804c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "article_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>WilliamKazer</td>\n",
              "      <td>United States China verge breakthrough testy t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>SimonCowell</td>\n",
              "      <td>Lloyd London announce Tuesday backer time meet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>DarrenSchuettler</td>\n",
              "      <td>Canada security regulator say Thursday probe B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>968</th>\n",
              "      <td>TheresePoletti</td>\n",
              "      <td>Microsoft Corp. big corporate foe include Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>JonathanBirt</td>\n",
              "      <td>drug discovery company Chiroscience Group Plc ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                author                                               text\n",
              "925       WilliamKazer  United States China verge breakthrough testy t...\n",
              "584        SimonCowell  Lloyd London announce Tuesday backer time meet...\n",
              "1000  DarrenSchuettler  Canada security regulator say Thursday probe B...\n",
              "968     TheresePoletti  Microsoft Corp. big corporate foe include Inte...\n",
              "1600      JonathanBirt  drug discovery company Chiroscience Group Plc ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aZZcgY0zVY_"
      },
      "source": [
        "### Split the sample into a training and test sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Gr-Y1fzVZA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_m0XbFXzVZD",
        "outputId": "bd2c77ee-c944-4db4-9d51-9c7c2c803e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VuusccEzVZG"
      },
      "source": [
        "### Train and evaluate function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKkWHuvZzVZI"
      },
      "source": [
        "Simple function to train (i.e. fit) and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASW0B6sjzVZI"
      },
      "source": [
        "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Accuracy on training set:\")\n",
        "    print(clf.score(X_train, y_train))\n",
        "    print(\"Accuracy on testing set:\")\n",
        "    print(clf.score(X_test, y_test))\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "uSwaI8sdzVZW"
      },
      "source": [
        "### <span>Naïve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3l7pDZpzVZX"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXsEg9LXzVZZ"
      },
      "source": [
        "Define pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O29a1MF5zVZZ"
      },
      "source": [
        "clf = Pipeline([\n",
        "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
        "                             lowercase = True,\n",
        "                            max_features = 1500,\n",
        "                            stop_words='english'\n",
        "                            )),\n",
        "\n",
        "    ('clf', MultinomialNB(alpha = 1,\n",
        "                          fit_prior = True\n",
        "                          )\n",
        "    ),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdfKwR-3zVZc"
      },
      "source": [
        "Train and show evaluation stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5R-dmEqzVZq",
        "outputId": "e63b90dd-0c4d-43f6-d0a7-4dab398141b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set:\n",
            "0.8405\n",
            "Accuracy on testing set:\n",
            "0.696\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "    AaronPressman       0.88      0.70      0.78        10\n",
            "       AlanCrosby       0.55      0.67      0.60         9\n",
            "   AlexanderSmith       0.71      0.56      0.63         9\n",
            "  BenjaminKangLim       0.67      0.18      0.29        11\n",
            "    BernardHickey       0.75      0.67      0.71         9\n",
            "      BradDorfman       0.90      0.90      0.90        10\n",
            " DarrenSchuettler       0.73      0.62      0.67        13\n",
            "      DavidLawder       0.89      0.62      0.73        13\n",
            "    EdnaFernandes       1.00      0.41      0.58        17\n",
            "      EricAuchard       0.80      0.44      0.57         9\n",
            "   FumikoFujisaki       0.86      0.86      0.86         7\n",
            "   GrahamEarnshaw       0.60      1.00      0.75         9\n",
            " HeatherScoffield       1.00      0.58      0.74        12\n",
            "       JanLopatka       0.62      0.45      0.53        11\n",
            "    JaneMacartney       0.40      0.50      0.44         8\n",
            "     JimGilchrist       0.92      1.00      0.96        12\n",
            "   JoWinterbottom       0.64      0.90      0.75        10\n",
            "         JoeOrtiz       0.70      0.88      0.78         8\n",
            "     JohnMastrini       0.58      0.78      0.67         9\n",
            "     JonathanBirt       0.83      0.71      0.77        14\n",
            "      KarlPenhaul       1.00      1.00      1.00        10\n",
            "        KeithWeir       0.45      0.83      0.59         6\n",
            "   KevinDrawbaugh       0.20      0.40      0.27         5\n",
            "    KevinMorrison       0.71      0.91      0.80        11\n",
            "    KirstinRidley       0.64      0.64      0.64        14\n",
            "KouroshKarimkhany       0.90      0.90      0.90        10\n",
            "        LydiaZajc       0.58      0.78      0.67         9\n",
            "   LynneO'Donnell       0.86      0.86      0.86        14\n",
            "  LynnleyBrowning       0.91      1.00      0.95        10\n",
            "  MarcelMichelson       0.67      0.80      0.73        10\n",
            "     MarkBendeich       0.86      0.67      0.75         9\n",
            "       MartinWolk       0.67      0.20      0.31        10\n",
            "     MatthewBunce       1.00      1.00      1.00        10\n",
            "    MichaelConnor       0.67      0.60      0.63        10\n",
            "       MureDickie       0.67      0.46      0.55        13\n",
            "        NickLouth       0.64      0.90      0.75        10\n",
            "  PatriciaCommins       0.57      0.89      0.70         9\n",
            "    PeterHumphrey       0.44      0.70      0.54        10\n",
            "       PierreTran       1.00      0.62      0.76        13\n",
            "       RobinSidel       1.00      0.78      0.88         9\n",
            "     RogerFillion       1.00      0.88      0.93         8\n",
            "      SamuelPerry       0.54      0.64      0.58        11\n",
            "     SarahDavison       0.83      0.42      0.56        12\n",
            "      ScottHillis       0.22      0.67      0.33         3\n",
            "      SimonCowell       0.71      0.50      0.59        10\n",
            "         TanEeLyn       0.56      0.62      0.59         8\n",
            "   TheresePoletti       0.83      1.00      0.91        10\n",
            "       TimFarrand       0.60      1.00      0.75         9\n",
            "       ToddNissen       0.53      0.89      0.67         9\n",
            "     WilliamKazer       0.20      0.12      0.15         8\n",
            "\n",
            "        micro avg       0.70      0.70      0.70       500\n",
            "        macro avg       0.71      0.70      0.68       500\n",
            "     weighted avg       0.74      0.70      0.69       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ximtzrGXzVaJ"
      },
      "source": [
        "Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDZnSjVczVaJ",
        "outputId": "9357f7ab-b449-4534-e0e4-d6eecd962104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "joblib.dump(clf, 'naive_bayes_results.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['naive_bayes_results.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWUzMTi0zVaN"
      },
      "source": [
        "Predict out of sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDAtLlu8zVaO"
      },
      "source": [
        "example_y, example_X = y_train[33], X_train[33]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE1Kcc5HzVaU",
        "outputId": "9bcfe54c-9e47-4f7f-b764-ff0b95b64a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Actual author:', example_y)\n",
        "print('Predicted author:', clf.predict([example_X])[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual author: EricAuchard\n",
            "Predicted author: EricAuchard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zEih7o-RzVaa"
      },
      "source": [
        "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3tWA3FizVab"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk2xP8ZdzVal"
      },
      "source": [
        "Define pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtWDmCKFzVal"
      },
      "source": [
        "clf_svm = Pipeline([\n",
        "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
        "                             lowercase = True,\n",
        "                            max_features = 1500,\n",
        "                            stop_words='english'\n",
        "                            )),\n",
        "\n",
        "    ('clf', SVC(kernel='rbf' ,\n",
        "                C=10, gamma=0.3)\n",
        "    ),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnKn1VfzVao"
      },
      "source": [
        "*Note:* The SVC estimator is very sensitive to the hyperparameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMbZQU7zVas"
      },
      "source": [
        "Train and show evaluation stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYAILjS5zVat",
        "outputId": "980ffe75-c52e-4d2b-8b93-af020b1046ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set:\n",
            "0.998\n",
            "Accuracy on testing set:\n",
            "0.828\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "    AaronPressman       0.82      0.90      0.86        10\n",
            "       AlanCrosby       0.78      0.78      0.78         9\n",
            "   AlexanderSmith       1.00      0.78      0.88         9\n",
            "  BenjaminKangLim       0.78      0.64      0.70        11\n",
            "    BernardHickey       1.00      0.89      0.94         9\n",
            "      BradDorfman       0.91      1.00      0.95        10\n",
            " DarrenSchuettler       1.00      1.00      1.00        13\n",
            "      DavidLawder       0.82      0.69      0.75        13\n",
            "    EdnaFernandes       1.00      0.82      0.90        17\n",
            "      EricAuchard       0.62      0.89      0.73         9\n",
            "   FumikoFujisaki       1.00      0.86      0.92         7\n",
            "   GrahamEarnshaw       0.73      0.89      0.80         9\n",
            " HeatherScoffield       0.92      1.00      0.96        12\n",
            "       JanLopatka       0.78      0.64      0.70        11\n",
            "    JaneMacartney       0.50      0.38      0.43         8\n",
            "     JimGilchrist       1.00      0.92      0.96        12\n",
            "   JoWinterbottom       0.75      0.90      0.82        10\n",
            "         JoeOrtiz       0.78      0.88      0.82         8\n",
            "     JohnMastrini       0.67      0.89      0.76         9\n",
            "     JonathanBirt       0.93      0.93      0.93        14\n",
            "      KarlPenhaul       1.00      1.00      1.00        10\n",
            "        KeithWeir       1.00      0.67      0.80         6\n",
            "   KevinDrawbaugh       0.56      1.00      0.71         5\n",
            "    KevinMorrison       0.85      1.00      0.92        11\n",
            "    KirstinRidley       0.92      0.79      0.85        14\n",
            "KouroshKarimkhany       0.89      0.80      0.84        10\n",
            "        LydiaZajc       1.00      0.78      0.88         9\n",
            "   LynneO'Donnell       0.93      0.93      0.93        14\n",
            "  LynnleyBrowning       1.00      1.00      1.00        10\n",
            "  MarcelMichelson       0.83      1.00      0.91        10\n",
            "     MarkBendeich       0.90      1.00      0.95         9\n",
            "       MartinWolk       0.70      0.70      0.70        10\n",
            "     MatthewBunce       1.00      1.00      1.00        10\n",
            "    MichaelConnor       0.82      0.90      0.86        10\n",
            "       MureDickie       0.78      0.54      0.64        13\n",
            "        NickLouth       0.90      0.90      0.90        10\n",
            "  PatriciaCommins       1.00      1.00      1.00         9\n",
            "    PeterHumphrey       0.45      0.50      0.48        10\n",
            "       PierreTran       1.00      0.85      0.92        13\n",
            "       RobinSidel       0.90      1.00      0.95         9\n",
            "     RogerFillion       1.00      0.88      0.93         8\n",
            "      SamuelPerry       0.88      0.64      0.74        11\n",
            "     SarahDavison       0.56      0.42      0.48        12\n",
            "      ScottHillis       0.43      1.00      0.60         3\n",
            "      SimonCowell       1.00      0.70      0.82        10\n",
            "         TanEeLyn       0.67      0.75      0.71         8\n",
            "   TheresePoletti       1.00      1.00      1.00        10\n",
            "       TimFarrand       0.64      1.00      0.78         9\n",
            "       ToddNissen       0.62      0.56      0.59         9\n",
            "     WilliamKazer       0.45      0.62      0.53         8\n",
            "\n",
            "        micro avg       0.83      0.83      0.83       500\n",
            "        macro avg       0.83      0.83      0.82       500\n",
            "     weighted avg       0.84      0.83      0.83       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6wlU59zVaw"
      },
      "source": [
        "Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJQSupEhzVax",
        "outputId": "92ef1d98-f274-4766-a775-e6a9c0594572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "joblib.dump(clf_svm, 'svm_results.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_results.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWGWAhmSzVa6"
      },
      "source": [
        "Predict out of sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b17BsOqOzVa7"
      },
      "source": [
        "example_y, example_X = y_train[33], X_train[33]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn2k9wDHzVa-",
        "outputId": "26437c3b-869d-4787-c177-666bde1c8b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Actual author:', example_y)\n",
        "print('Predicted author:', clf_svm.predict([example_X])[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual author: EricAuchard\n",
            "Predicted author: EricAuchard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "vpRZqHSNzVbB"
      },
      "source": [
        "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiQ52YFLzVbC"
      },
      "source": [
        "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
        "\n",
        "It can be difficult to figure out what the best parameters are.\n",
        "\n",
        "We can use `GridSearchCV` to help figure this out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4wtBtFEzVbC"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJgG3nczVbH"
      },
      "source": [
        "First we define the options that should be tried out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBSoERHDzVbJ"
      },
      "source": [
        "clf_search = Pipeline([\n",
        "    ('vect', TfidfVectorizer()),\n",
        "    ('clf', SVC())\n",
        "])\n",
        "parameters = { 'vect__stop_words': ['english'],\n",
        "                'vect__strip_accents': ['unicode'],\n",
        "              'vect__max_features' : [1500],\n",
        "              'vect__ngram_range': [(1,1), (2,2) ],\n",
        "             'clf__gamma' : [0.2, 0.3, 0.4],\n",
        "             'clf__C' : [8, 10, 12],\n",
        "              'clf__kernel' : ['rbf']\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp-NLigBzVbL"
      },
      "source": [
        "Run everything:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1VQ-VGjzVbM",
        "outputId": "9a509d22-165c-42c1-8d05-ecff30cd59b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "grid = GridSearchCV(clf_search, param_grid=parameters, scoring=make_scorer(f1_score, average='micro'), n_jobs=1)\n",
        "grid.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt2CvCxVzVbQ"
      },
      "source": [
        "*Note:* if you are on a powerful unix system you can set n_jobs to the number of available threads to speed up the calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZg7EZTBzVbQ"
      },
      "source": [
        "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
        "y_true, y_pred = y_test, grid.predict(X_test)\n",
        "print(metrics.classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NvIi77ZczVbW"
      },
      "source": [
        "## <span>Unsupervised</span><a id='trad_ml_unsupervised'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yGyPcY1JzVbh"
      },
      "source": [
        "### <span>Latent Dirichilet Allocation (LDA)</span><a id='trad_ml_unsupervised_lda'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeVuhs6ZzVbh"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEfe9WZ-zVbp"
      },
      "source": [
        "Vectorizer (using countvectorizer for the sake of example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4JnRCXzVbp"
      },
      "source": [
        "vectorizer = CountVectorizer(strip_accents='unicode',\n",
        "                             lowercase = True,\n",
        "                            max_features = 1500,\n",
        "                            stop_words='english', max_df=0.8)\n",
        "tf_large = vectorizer.fit_transform(clean_paragraphs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC05UAkVzVbs"
      },
      "source": [
        "Run the LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crHxv3OKzVbs"
      },
      "source": [
        "n_topics = 10\n",
        "n_top_words = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ECCbQnczVbx"
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10,\n",
        "                                learning_method='online',\n",
        "                                n_jobs=1)\n",
        "lda_fitted = lda.fit_transform(tf_large)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WWs2PfDzVb0"
      },
      "source": [
        "Visualize top words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHgccEPWzVb2"
      },
      "source": [
        "def save_top_words(model, feature_names, n_top_words):\n",
        "    out_list = []\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        out_list.append((topic_idx+1, \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
        "    out_df = pd.DataFrame(out_list, columns=['topic_id', 'top_words'])\n",
        "    return out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUApo9RHzVb_"
      },
      "source": [
        "result_df = save_top_words(lda, vectorizer.get_feature_names(), n_top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXjV_kHYzVcC"
      },
      "source": [
        "result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "rtrOd5e2zVci"
      },
      "source": [
        "### <span>pyLDAvis</span><a id='trad_ml_unsupervised_pyLDAvis'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS_91lazzVck"
      },
      "source": [
        "%matplotlib inline\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rxaaZ9fzVco"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(lda, tf_large, vectorizer, n_jobs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqWw0uej5wo-"
      },
      "source": [
        "\n",
        "Credit: [Ties de Kok](https://github.com/TiesdeKok)\n",
        "\n",
        "Repository: [Python NLP](https://github.com/TiesdeKok/Python_NLP_Tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyoPQUDx5ylY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}